{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e844f6b0-e642-40d7-8862-5c4a5c3b1ac4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Real world datasets have surprises in them such as missing data, outliers, and data entry errors. Code you create to analyze this data has its own set of surprises such as logical errors, syntax errors and conceptual errors. In the interest of progress it can be helpful to reduce variables to just your code before introducing additional complexity. In this example we will generate a synthetic dataset and form a base regression model for the task.\n",
    "\n",
    "Based on the approach described in https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3c8fd-18d3-4f38-bbfc-16a0ad6f1033",
   "metadata": {},
   "source": [
    "# Generating Data\n",
    "\n",
    "We'll generate data from a pure linear function and then pollute the data with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08df8a7f-6e9d-4595-87f2-54649f38784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# y = Xw + b + noise\n",
    "class SyntheticRegressionData():\n",
    "    def __init__(self, w, b, noise=0.1, training_count = 1000, validation_count = 1000, batch_size = 64):\n",
    "        self.batch_size = batch_size\n",
    "        self.training_count = training_count\n",
    "        self.validation_count = validation_count\n",
    "        self.observation_count = training_count + validation_count\n",
    "        self.X = torch.randn(self.observation_count, len(w))\n",
    "        noise = torch.randn(self.observation_count, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=train)\n",
    "    \n",
    "    def get_dataloader(self, train):\n",
    "        if train:\n",
    "            # Training data is in the front of the dataset\n",
    "            i = slice(0, self.training_count)\n",
    "        else:\n",
    "            # Validation data is at the end of the dataset\n",
    "            i = slice(self.training_count, None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)\n",
    "    \n",
    "    def training_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5add6ad-d466-43d3-829e-ac1bbd70e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([ 0.2400, -2.2767])\n",
      "Label: tensor([8.9570])\n"
     ]
    }
   ],
   "source": [
    "# Try our new class\n",
    "data = SyntheticRegressionData(w=torch.tensor([5, -2.1]), b=3.1)\n",
    "print(f\"Features: {data.X[0]}\")\n",
    "print(f\"Label: {data.y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16cc2094-7c3a-436a-8bad-a4ed8c33c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 2])\n",
      "y shape: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# Try out the training dataloader\n",
    "X, y = next(iter(data.training_dataloader()))\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330130e-8990-4eb8-bd27-f04c15e1f1fa",
   "metadata": {},
   "source": [
    "# Define the Optimizer (From Scratch)\n",
    "\n",
    "As descibed in the article d2l.ai, we'll use mini-batch stochastic gradient descent for our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ef0752-709b-41cc-bbf0-08647b857545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, params, learning_rate):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad_zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631bc58f-0180-4e24-b3c2-f4434ddcfffb",
   "metadata": {},
   "source": [
    "# Define the Model (From Scratch)\n",
    "\n",
    "We'll use only primitives for the first model and then we'll reimplement using more features in the Torch framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103179b8-f6fa-4733-b601-9af72e1f209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.import nn\n",
    "\n",
    "class LinearRegressionV1(torch.nn):\n",
    "    def __init__(self, input_count, learning_rate, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        # Draw data from the normal distribution centered at 0\n",
    "        # with standard deviation sigma\n",
    "        self.w = torch.normal(0, sigma, (input_count, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # y = Xw + b\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "\n",
    "    def loss(self, y_predicted, y):\n",
    "        # We use the squared loss function\n",
    "        l = ((y_predicted - y) ** 2) / 2\n",
    "        return l.mean()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD([self.w, self.b], self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de476df-51d9-4144-9cf0-973bf4f66900",
   "metadata": {},
   "source": [
    "# Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
