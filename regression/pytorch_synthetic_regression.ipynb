{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e844f6b0-e642-40d7-8862-5c4a5c3b1ac4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Real world datasets have surprises in them such as missing data, outliers, and data entry errors. Code you create to analyze this data has its own set of surprises such as logical errors, syntax errors and conceptual errors. In the interest of progress it can be helpful to reduce variables to just your code before introducing additional complexity. In this example we will generate a synthetic dataset and form a base regression model for the task.\n",
    "\n",
    "This notebook will cover a from scratch implementation and a more concise implementation.\n",
    "\n",
    "Based on the approach described in https://d2l.ai/chapter_linear-regression/synthetic-regression-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3c8fd-18d3-4f38-bbfc-16a0ad6f1033",
   "metadata": {},
   "source": [
    "# Generating Data\n",
    "\n",
    "We'll generate data from a pure linear function and then pollute the data with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08df8a7f-6e9d-4595-87f2-54649f38784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# y = Xw + b + noise\n",
    "class SyntheticRegressionData():\n",
    "    def __init__(self, w, b, noise=0.1, training_count = 1000, validation_count = 1000, batch_size = 32):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.batch_size = batch_size\n",
    "        self.training_count = training_count\n",
    "        self.validation_count = validation_count\n",
    "        self.observation_count = training_count + validation_count\n",
    "        self.X = torch.randn(self.observation_count, len(w))\n",
    "        noise = torch.randn(self.observation_count, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=train)\n",
    "    \n",
    "    def get_dataloader(self, train):\n",
    "        if train:\n",
    "            # Training data is in the front of the dataset\n",
    "            i = slice(0, self.training_count)\n",
    "        else:\n",
    "            # Validation data is at the end of the dataset\n",
    "            i = slice(self.training_count, None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)\n",
    "    \n",
    "    def training_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5add6ad-d466-43d3-829e-ac1bbd70e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([-0.6643,  1.7131])\n",
      "Label: tensor([-3.8266])\n"
     ]
    }
   ],
   "source": [
    "# Try our new class\n",
    "data = SyntheticRegressionData(w=torch.tensor([5, -2.1]), b=3.1)\n",
    "print(f\"Features: {data.X[0]}\")\n",
    "print(f\"Label: {data.y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16cc2094-7c3a-436a-8bad-a4ed8c33c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2])\n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Try out the training dataloader\n",
    "X, y = next(iter(data.training_dataloader()))\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330130e-8990-4eb8-bd27-f04c15e1f1fa",
   "metadata": {},
   "source": [
    "# Define the Optimizer (From Scratch)\n",
    "\n",
    "As descibed in the article d2l.ai, we'll use mini-batch stochastic gradient descent for our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ef0752-709b-41cc-bbf0-08647b857545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, params, learning_rate):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631bc58f-0180-4e24-b3c2-f4434ddcfffb",
   "metadata": {},
   "source": [
    "# Define the Model (From Scratch)\n",
    "\n",
    "We'll use only primitives for the first model and then we'll reimplement using more features in the Torch framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103179b8-f6fa-4733-b601-9af72e1f209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearRegressionV1(torch.nn.Module):\n",
    "    def __init__(self, input_count, learning_rate, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        # Draw data from the normal distribution centered at 0\n",
    "        # with standard deviation sigma\n",
    "        self.w = torch.normal(0, sigma, (input_count, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # y = Xw + b\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "\n",
    "    def loss(self, y_predicted, y):\n",
    "        # We use the squared loss function\n",
    "        l = ((y_predicted - y) ** 2) / 2\n",
    "        return l.mean()\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        return l\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD([self.w, self.b], self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de476df-51d9-4144-9cf0-973bf4f66900",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In each epoch, we process an iteration of a random batch of data. We continue processing iterations until all data has been seen.\n",
    "\n",
    "For each iteration we do the following\n",
    " 1. Grab a batch of training samples\n",
    " 2. Compute the loss\n",
    " 3. Compute the gradient on where to move next\n",
    " 4. Update the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4792915-f4cf-4b9c-970f-a91ecba1cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchTrainer():\n",
    "    def __init__(self, max_epochs):\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.training_dataloader = data.training_dataloader()\n",
    "        self.validation_dataloader = data.validation_dataloader()\n",
    "        self.training_batch_count = len(self.training_dataloader)\n",
    "        self.validation_batch_count = len(self.validation_dataloader)\n",
    "\n",
    "    def prepare_model(self, data):\n",
    "        model.trainer = self\n",
    "        self.model = model\n",
    "\n",
    "    def fit_epoch(self, _callback = None):\n",
    "        self.model.train()\n",
    "        for batch in self.training_dataloader:\n",
    "            loss = self.model.training_step(batch)\n",
    "            if _callback:\n",
    "                _callback(\"training loss\", loss)\n",
    "            self.optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            self.training_batch_index += 1\n",
    "\n",
    "        self.model.eval()\n",
    "        for batch in self.validation_dataloader:\n",
    "            with torch.no_grad():\n",
    "                loss = self.model.validation_step(batch)\n",
    "                if _callback:\n",
    "                    _callback(\"validation loss\", loss)\n",
    "            self.validation_batch_index +=1\n",
    "    \n",
    "    def fit(self, model, data, _callback = None):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optimizer = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.training_batch_index = 0\n",
    "        self.validation_batch_index = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            _callback(\"epoch\", self.epoch)\n",
    "            self.fit_epoch(_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89680b-06aa-452a-b261-fa01d1d0d0d8",
   "metadata": {},
   "source": [
    "# All Together\n",
    "\n",
    "We'll bring it all together to declare the model, generate synthetic data and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8466a878-7655-4f1a-a725-d380eefa995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training loss: 14.55424690246582\n",
      "training loss: 20.985332489013672\n",
      "training loss: 16.794666290283203\n",
      "training loss: 22.17304229736328\n",
      "training loss: 10.992134094238281\n",
      "training loss: 9.377906799316406\n",
      "training loss: 17.01113510131836\n",
      "training loss: 12.273624420166016\n",
      "validation loss: 6.824306964874268\n",
      "validation loss: 15.008211135864258\n",
      "epoch: 1\n",
      "training loss: 8.786096572875977\n",
      "training loss: 9.284029006958008\n",
      "training loss: 11.024335861206055\n",
      "training loss: 8.116580963134766\n",
      "training loss: 8.700980186462402\n",
      "training loss: 12.679927825927734\n",
      "training loss: 9.02916431427002\n",
      "training loss: 9.526836395263672\n",
      "validation loss: 4.251908302307129\n",
      "validation loss: 9.358114242553711\n",
      "epoch: 2\n",
      "training loss: 11.314159393310547\n",
      "training loss: 6.041918754577637\n",
      "training loss: 5.531486988067627\n",
      "training loss: 5.653379917144775\n",
      "training loss: 4.798514366149902\n",
      "training loss: 4.812900543212891\n",
      "training loss: 4.136332035064697\n",
      "training loss: 5.866523265838623\n",
      "validation loss: 2.6649060249328613\n",
      "validation loss: 5.856570720672607\n",
      "epoch: 3\n",
      "training loss: 3.912876605987549\n",
      "training loss: 2.625282049179077\n",
      "training loss: 4.780890941619873\n",
      "training loss: 5.000694751739502\n",
      "training loss: 4.103759288787842\n",
      "training loss: 3.1260294914245605\n",
      "training loss: 3.150686025619507\n",
      "training loss: 3.4359867572784424\n",
      "validation loss: 1.6801164150238037\n",
      "validation loss: 3.6785919666290283\n",
      "epoch: 4\n",
      "training loss: 3.1019861698150635\n",
      "training loss: 3.162303924560547\n",
      "training loss: 3.222517490386963\n",
      "training loss: 2.1436281204223633\n",
      "training loss: 2.2525463104248047\n",
      "training loss: 2.0705976486206055\n",
      "training loss: 1.4506293535232544\n",
      "training loss: 1.5444672107696533\n",
      "validation loss: 1.0658726692199707\n",
      "validation loss: 2.3193070888519287\n",
      "epoch: 5\n",
      "training loss: 1.4740558862686157\n",
      "training loss: 1.9170780181884766\n",
      "training loss: 1.2927652597427368\n",
      "training loss: 1.7610689401626587\n",
      "training loss: 1.4235209226608276\n",
      "training loss: 1.598878264427185\n",
      "training loss: 1.5575083494186401\n",
      "training loss: 0.9315122365951538\n",
      "validation loss: 0.6807883381843567\n",
      "validation loss: 1.4682717323303223\n",
      "epoch: 6\n",
      "training loss: 1.0299909114837646\n",
      "training loss: 1.232890009880066\n",
      "training loss: 0.6865159273147583\n",
      "training loss: 0.891260027885437\n",
      "training loss: 0.8948582410812378\n",
      "training loss: 1.0460119247436523\n",
      "training loss: 0.7380226850509644\n",
      "training loss: 1.0669806003570557\n",
      "validation loss: 0.4377131760120392\n",
      "validation loss: 0.9329432845115662\n",
      "epoch: 7\n",
      "training loss: 0.8112743496894836\n",
      "training loss: 0.9903708100318909\n",
      "training loss: 0.5453482270240784\n",
      "training loss: 0.4948895573616028\n",
      "training loss: 0.5923815965652466\n",
      "training loss: 0.45768943428993225\n",
      "training loss: 0.3498243987560272\n",
      "training loss: 0.5978873372077942\n",
      "validation loss: 0.2836091220378876\n",
      "validation loss: 0.5956457853317261\n",
      "epoch: 8\n",
      "training loss: 0.5260432362556458\n",
      "training loss: 0.4458520710468292\n",
      "training loss: 0.5352301597595215\n",
      "training loss: 0.25539249181747437\n",
      "training loss: 0.33437541127204895\n",
      "training loss: 0.3097762167453766\n",
      "training loss: 0.48271024227142334\n",
      "training loss: 0.2153938114643097\n",
      "validation loss: 0.18517139554023743\n",
      "validation loss: 0.3819311261177063\n",
      "epoch: 9\n",
      "training loss: 0.2698160707950592\n",
      "training loss: 0.23100225627422333\n",
      "training loss: 0.48509979248046875\n",
      "training loss: 0.15820558369159698\n",
      "training loss: 0.30021631717681885\n",
      "training loss: 0.20272594690322876\n",
      "training loss: 0.15829946100711823\n",
      "training loss: 0.1962628811597824\n",
      "validation loss: 0.12208610773086548\n",
      "validation loss: 0.2463277131319046\n"
     ]
    }
   ],
   "source": [
    "def status_callback(topic, status):\n",
    "    print(f\"{topic}: {status}\")\n",
    "\n",
    "model = LinearRegressionV1(2, learning_rate=0.03)\n",
    "data = SyntheticRegressionData(w=torch.tensor([5, -2.1]), b=3.1, training_count=256, validation_count=64)\n",
    "trainer = MiniBatchTrainer(max_epochs=10)\n",
    "trainer.fit(model, data, status_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "431cb573-81bc-4f7d-b713-8135382544b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in estimating w: tensor([ 0.6079, -0.1147])\n",
      "Error is estimating b: tensor([0.1834])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(f\"Error in estimating w: {data.w - model.w.reshape(data.w.shape)}\")\n",
    "    print(f\"Error is estimating b: {data.b - model.b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28615e-b128-40a6-b921-4ed85e4949ee",
   "metadata": {},
   "source": [
    "# Define the Model (Concise)\n",
    "\n",
    "Use the built in PyTorch classes to simplify our model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7501e25d-12fb-432f-9f10-3244385d0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearRegressionV2(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.net = nn.LazyLinear(1)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    def loss(self, y_predicted, y):\n",
    "        fn = nn.MSELoss()\n",
    "        return fn(y_predicted, y)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        return l\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), self.learning_rate)\n",
    "\n",
    "    def get_w_b(self):\n",
    "        return (self.net.weight.data, self.net.bias.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1580d5-f1f5-4c54-a337-a49613d20dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Github\\ml-reference-designs\\.venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training loss: 60.92463684082031\n",
      "training loss: 31.12190055847168\n",
      "training loss: 27.985639572143555\n",
      "training loss: 24.437040328979492\n",
      "training loss: 24.39147186279297\n",
      "training loss: 24.701032638549805\n",
      "training loss: 11.940518379211426\n",
      "training loss: 21.80307388305664\n",
      "validation loss: 22.4771785736084\n",
      "validation loss: 16.777027130126953\n",
      "epoch: 1\n",
      "training loss: 14.044628143310547\n",
      "training loss: 16.353137969970703\n",
      "training loss: 14.008424758911133\n",
      "training loss: 10.475452423095703\n",
      "training loss: 11.166021347045898\n",
      "training loss: 9.426080703735352\n",
      "training loss: 5.273672103881836\n",
      "training loss: 7.150327682495117\n",
      "validation loss: 8.76409912109375\n",
      "validation loss: 6.6105780601501465\n",
      "epoch: 2\n",
      "training loss: 4.891706943511963\n",
      "training loss: 7.176791191101074\n",
      "training loss: 5.13761568069458\n",
      "training loss: 3.826969861984253\n",
      "training loss: 5.051006317138672\n",
      "training loss: 2.134897232055664\n",
      "training loss: 3.538323402404785\n",
      "training loss: 2.7365832328796387\n",
      "validation loss: 3.430253505706787\n",
      "validation loss: 2.6206517219543457\n",
      "epoch: 3\n",
      "training loss: 2.599241256713867\n",
      "training loss: 1.839906096458435\n",
      "training loss: 2.2733497619628906\n",
      "training loss: 1.6074684858322144\n",
      "training loss: 1.4628732204437256\n",
      "training loss: 1.1477713584899902\n",
      "training loss: 1.299747347831726\n",
      "training loss: 1.3653644323349\n",
      "validation loss: 1.3505480289459229\n",
      "validation loss: 1.048526406288147\n",
      "epoch: 4\n",
      "training loss: 0.9818029403686523\n",
      "training loss: 0.9152162671089172\n",
      "training loss: 0.5874661803245544\n",
      "training loss: 0.9474660158157349\n",
      "training loss: 0.6115456223487854\n",
      "training loss: 0.4447370767593384\n",
      "training loss: 0.5838844180107117\n",
      "training loss: 0.3914071321487427\n",
      "validation loss: 0.5338618755340576\n",
      "validation loss: 0.4237414300441742\n",
      "epoch: 5\n",
      "training loss: 0.46280166506767273\n",
      "training loss: 0.34889620542526245\n",
      "training loss: 0.23413315415382385\n",
      "training loss: 0.29035311937332153\n",
      "training loss: 0.23130491375923157\n",
      "training loss: 0.2764017879962921\n",
      "training loss: 0.26082414388656616\n",
      "training loss: 0.12342270463705063\n",
      "validation loss: 0.21208041906356812\n",
      "validation loss: 0.17380250990390778\n",
      "epoch: 6\n",
      "training loss: 0.16132690012454987\n",
      "training loss: 0.16818496584892273\n",
      "training loss: 0.11604021489620209\n",
      "training loss: 0.12119895219802856\n",
      "training loss: 0.08535762876272202\n",
      "training loss: 0.07328283786773682\n",
      "training loss: 0.12404360622167587\n",
      "training loss: 0.09575632959604263\n",
      "validation loss: 0.08599171787500381\n",
      "validation loss: 0.07419799268245697\n",
      "epoch: 7\n",
      "training loss: 0.061219729483127594\n",
      "training loss: 0.07920773327350616\n",
      "training loss: 0.08641386032104492\n",
      "training loss: 0.036038074642419815\n",
      "training loss: 0.03797433152794838\n",
      "training loss: 0.05725782737135887\n",
      "training loss: 0.04694290831685066\n",
      "training loss: 0.02749011293053627\n",
      "validation loss: 0.03647122532129288\n",
      "validation loss: 0.03416460007429123\n",
      "epoch: 8\n",
      "training loss: 0.028318293392658234\n",
      "training loss: 0.03450814634561539\n",
      "training loss: 0.02920542284846306\n",
      "training loss: 0.025007903575897217\n",
      "training loss: 0.03150833770632744\n",
      "training loss: 0.024240024387836456\n",
      "training loss: 0.023359086364507675\n",
      "training loss: 0.029108144342899323\n",
      "validation loss: 0.017170386388897896\n",
      "validation loss: 0.018034374341368675\n",
      "epoch: 9\n",
      "training loss: 0.019151844084262848\n",
      "training loss: 0.032809868454933167\n",
      "training loss: 0.0195135697722435\n",
      "training loss: 0.008859865367412567\n",
      "training loss: 0.017338069155812263\n",
      "training loss: 0.017500020563602448\n",
      "training loss: 0.015251629054546356\n",
      "training loss: 0.01330728642642498\n",
      "validation loss: 0.009950648993253708\n",
      "validation loss: 0.011736927554011345\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionV2(learning_rate=0.03)\n",
    "data = SyntheticRegressionData(w=torch.tensor([5, -2.1]), b=3.1, training_count=256, validation_count=64)\n",
    "trainer = MiniBatchTrainer(max_epochs=10)\n",
    "trainer.fit(model, data, status_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb8eadb9-6e06-4efb-bc57-3ceb75272c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([ 0.0389, -0.0282])\n",
      "error in estimating b: tensor([0.0421])\n"
     ]
    }
   ],
   "source": [
    "w, b = model.get_w_b()\n",
    "\n",
    "print(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')\n",
    "print(f'error in estimating b: {data.b - b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
